cpu-bind=MASK - cn1145, task  0  0 [27110]: mask 0xffffffffffffffff set
-1.0989796330E+001
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: cn1145
  Location: mtl_ofi_component.c:928
  Error: No data available (61)
--------------------------------------------------------------------------
[cn1145.scarf.rl.ac.uk:27154] 119 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[cn1145.scarf.rl.ac.uk:27154] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[1679925301.899308] [cn1145:27194:0]          mpool.c:43   UCX  WARN  object 0x609ca00 was not returned to mpool ucp_requests
[1679925301.899331] [cn1145:27194:0]          mpool.c:43   UCX  WARN  object 0x609d100 was not returned to mpool ucp_requests
[1679925301.899442] [cn1145:27195:0]          mpool.c:43   UCX  WARN  object 0x5ca4740 was not returned to mpool ucp_requests
[1679925301.899466] [cn1145:27195:0]          mpool.c:43   UCX  WARN  object 0x5ca51c0 was not returned to mpool ucp_requests
[1679925301.899512] [cn1145:27196:0]          mpool.c:43   UCX  WARN  object 0x5a0db00 was not returned to mpool ucp_requests
[1679925301.899531] [cn1145:27196:0]          mpool.c:43   UCX  WARN  object 0x5a10c00 was not returned to mpool ucp_requests
[1679925301.899558] [cn1145:27184:0]          mpool.c:43   UCX  WARN  object 0x508a000 was not returned to mpool ucp_requests
[1679925301.899610] [cn1145:27185:0]          mpool.c:43   UCX  WARN  object 0x5ead400 was not returned to mpool ucp_requests
[1679925301.899602] [cn1145:27186:0]          mpool.c:43   UCX  WARN  object 0x6b2d180 was not returned to mpool ucp_requests
[1679925301.899654] [cn1145:27193:0]          mpool.c:43   UCX  WARN  object 0x67ccd80 was not returned to mpool ucp_requests
[1679925301.899697] [cn1145:27188:0]          mpool.c:43   UCX  WARN  object 0x6318a00 was not returned to mpool ucp_requests
[1679925301.899694] [cn1145:27190:0]          mpool.c:43   UCX  WARN  object 0x5c9a3c0 was not returned to mpool ucp_requests
[1679925301.899723] [cn1145:27191:0]          mpool.c:43   UCX  WARN  object 0x4f879c0 was not returned to mpool ucp_requests
[1679925301.899726] [cn1145:27192:0]          mpool.c:43   UCX  WARN  object 0x5a95c80 was not returned to mpool ucp_requests
[1679925301.899732] [cn1145:27180:0]          mpool.c:43   UCX  WARN  object 0x4e12940 was not returned to mpool ucp_requests
[1679925301.899731] [cn1145:27181:0]          mpool.c:43   UCX  WARN  object 0x5733000 was not returned to mpool ucp_requests
[1679925301.899734] [cn1145:27182:0]          mpool.c:43   UCX  WARN  object 0x6a95500 was not returned to mpool ucp_requests
[1679925301.899735] [cn1145:27183:0]          mpool.c:43   UCX  WARN  object 0x6a3ca00 was not returned to mpool ucp_requests
[1679925301.899751] [cn1145:27189:0]          mpool.c:43   UCX  WARN  object 0x50a0140 was not returned to mpool ucp_requests
[1679925301.901600] [cn1145:27187:0]          mpool.c:43   UCX  WARN  object 0x6dc8880 was not returned to mpool ucp_requests
[1679925301.904773] [cn1145:27179:0]          mpool.c:43   UCX  WARN  object 0x6c03f00 was not returned to mpool ucp_requests
-1.0989796330E+001
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: cn1145
  Location: mtl_ofi_component.c:928
  Error: No data available (61)
--------------------------------------------------------------------------
[cn1145.scarf.rl.ac.uk:28513] 119 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[cn1145.scarf.rl.ac.uk:28513] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
