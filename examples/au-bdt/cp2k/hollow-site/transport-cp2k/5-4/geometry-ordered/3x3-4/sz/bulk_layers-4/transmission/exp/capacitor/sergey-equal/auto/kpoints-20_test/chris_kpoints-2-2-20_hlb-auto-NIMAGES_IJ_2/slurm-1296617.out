cpu-bind=MASK - cn1061, task  0  0 [64019]: mask 0xffffffffffffffff set
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: cn1061
  Location: mtl_ofi_component.c:928
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[cn1061.scarf.rl.ac.uk:65014] 239 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[cn1061.scarf.rl.ac.uk:65014] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[cn1061.scarf.rl.ac.uk:65014] 239 more processes have sent help message help-mpi-api.txt / mpi-abort
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: cn1061
  Location: mtl_ofi_component.c:928
  Error: No data available (61)
--------------------------------------------------------------------------
[cn1061.scarf.rl.ac.uk:00400] 239 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[cn1061.scarf.rl.ac.uk:00400] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[1679497620.960256] [cn1061:500  :0]          mpool.c:43   UCX  WARN  object 0x6b9b840 was not returned to mpool ucp_requests
[1679497620.960310] [cn1061:493  :0]          mpool.c:43   UCX  WARN  object 0xc0ef200 was not returned to mpool ucp_requests
[1679497620.960334] [cn1061:496  :0]          mpool.c:43   UCX  WARN  object 0x523f200 was not returned to mpool ucp_requests
[1679497620.962226] [cn1061:471  :0]          mpool.c:43   UCX  WARN  object 0xb5d4840 was not returned to mpool ucp_requests
[1679497620.962218] [cn1061:483  :0]          mpool.c:43   UCX  WARN  object 0x529dec0 was not returned to mpool ucp_requests
[1679497620.962233] [cn1061:487  :0]          mpool.c:43   UCX  WARN  object 0xb49dd40 was not returned to mpool ucp_requests
[1679497620.962257] [cn1061:480  :0]          mpool.c:43   UCX  WARN  object 0x63bb9c0 was not returned to mpool ucp_requests
[1679497620.962257] [cn1061:492  :0]          mpool.c:43   UCX  WARN  object 0xadf2140 was not returned to mpool ucp_requests
[1679497620.962268] [cn1061:473  :0]          mpool.c:43   UCX  WARN  object 0xb440540 was not returned to mpool ucp_requests
[1679497620.962350] [cn1061:477  :0]          mpool.c:43   UCX  WARN  object 0x64bdfc0 was not returned to mpool ucp_requests
grep: bulk-VH_AV.dat: No such file or directory

--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: cn1061
  Location: mtl_ofi_component.c:928
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[cn1061.scarf.rl.ac.uk:01059] 239 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[cn1061.scarf.rl.ac.uk:01059] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[cn1061.scarf.rl.ac.uk:01059] 239 more processes have sent help message help-mpi-api.txt / mpi-abort
grep: bulk-VH_AV.dat: No such file or directory

--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: cn1061
  Location: mtl_ofi_component.c:928
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[cn1061.scarf.rl.ac.uk:01725] 239 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[cn1061.scarf.rl.ac.uk:01725] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[cn1061.scarf.rl.ac.uk:01725] 239 more processes have sent help message help-mpi-api.txt / mpi-abort
